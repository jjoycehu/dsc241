---
title: "project-joshua"
output: html_notebook
---

```{r}
# load packages
library(car)
library(rpart)
library(caret)        
library(Metrics)
library(rpart.plot)
library(randomForest)

# load data
df <- read.csv("../insurance.csv")
head(df)
```

## Model Selection
To model medical expenses as a function of customer attributes, we start with a basic multiple linear regression model:
\begin{equation}
    y_i = \beta_0 + \beta_1\cdot age_i + \beta_2\cdot sex_i+\beta_3\cdot BMI_i+ \beta_4\cdot children_i+\beta_5\cdot smoker_i+\beta_6\cdot region_i
\end{equation}
We fit this model on the data, minimizing teh residual sum of squares (RSS) and optimizing coefficient values:
\begin{equation}
    min_\beta\sum^n_{i=1}(y_i-x_i^T\beta)^2
\end{equation}
This model establishes the baseline performance and identifies major linear relationships. It is simple and easily interpretable but may underfit if nonlinearity or interactions exist. By minimizing prediction error, the goal is to accurately estimate medical expenses for future patients. This is important for accurate risk assessment and pricing for new customers that seek insurance coverage. By comparing the weights $(\beta_1, \dots, \beta_k)$, we can easily interpret the contributions each factor has on medical expenses. The model coefficients can represent the relationships between each patient characteristic and the patient's medical expenses. This inference analysis provides better understanding of which features drive medical expenses. 

To better capture complex relationships, the model may be extended to include interaction terms (e.g. $smoker\times BMI$) and polynomial non-linear transformations (e.g. $age^2$). This may possibly improve model performance, especially if the assumption of linearity is violated. However, this is at the cost of increased model complexity and risk of overfitting. This can be given by the following regression model:
\begin{equation}
    y = \beta_0 + \sum^p_{j=1}\beta_jx_j+\sum_{j<k}\beta_{xj}x_jx_k
\end{equation}

Logistic regression is another model we consider. This model is used for classification, not regression, but this can be helpful for predicting categories like high-risk, high-cost customers.

Another model we consider is a regression tree. This model also captures nonlinearity and interactions, is easy to visualize and explain, but is prone to overfitting without pruning and is less stable. An extension of this is the random forest model, a supervised learning algorithm that builds many trees, where each tree sees a random sample or subset of data and features, and predictions are averaged across the trees.

Pre-fitting Model Assumptions

```{r}
library(ggplot2)
library(car)        # For VIF
library(psych)      # For descriptive stats
library(dplyr)      # For data manipulation
library(GGally)     # For pair plots
library(olsrr)
```


```{r}
# Linearity

ggpairs(df, columns = c("age", "bmi", "children", "charges"))

# Scatterplots
ggplot(df, aes(x = bmi, y = charges)) + geom_point() + geom_smooth(method = "lm")
ggplot(df, aes(x = age, y = charges)) + geom_point() + geom_smooth(method = "lm")
```

```{r}
# Homoscedasticity
model <- lm(charges ~ age + bmi + children + sex + smoker + region, data = df)

# Residuals vs fitted values plot
plot(model$fitted.values, resid(model),
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")
```

```{r}
# Normality of Residuals
hist(resid(model), main = "Histogram of Residuals", xlab = "Residuals")

qqnorm(resid(model))
qqline(resid(model), col = "red")
```
```{r}
# Multicollinearity
vif(model)
```
```{r}
# Outliers/Skew in y
ggplot(df, aes(x = charges)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Medical Charges")

# Boxplot to check for outliers
ggplot(df, aes(y = charges)) + 
  geom_boxplot(fill = "tomato") + 
  labs(title = "Boxplot of Medical Charges")
```
```{r}
# Balanced Categorical Variables
table(df$sex)
table(df$smoker)
table(df$region)
```


Regression Tree Model

```{r}
df$sex <- as.factor(df$sex)
df$smoker <- as.factor(df$smoker)
df$region <- as.factor(df$region)

train_index <- createDataPartition(df$charges, p = 0.8, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

model <- rpart(charges ~ ., data = train_data, method = "anova")

preds <- predict(model, newdata = test_data)

rmse_val <- rmse(test_data$charges, preds)
ss_res <- sum((test_data$charges - preds)^2)
ss_tot <- sum((test_data$charges - mean(test_data$charges))^2)
r_squared <- 1 - (ss_res / ss_tot)

cat("Test RMSE:", rmse_val, "\n")
cat("Test R-squared:", r_squared, "\n")

rpart.plot(model, type = 2, extra = 101, fallen.leaves = TRUE)

plot(test_data$charges, preds, main = "Actual vs Predicted",
     xlab = "Actual Charges", ylab = "Predicted Charges")
abline(0, 1, col = "red")
```

Deeper Regression Tree Model

```{r}
model <- rpart(charges ~ ., data = train_data, method = "anova", control = rpart.control(minsplit = 5, cp = 0.001, maxdepth = 10))

preds <- predict(model, newdata = test_data)

rmse_val <- rmse(test_data$charges, preds)
ss_res <- sum((test_data$charges - preds)^2)
ss_tot <- sum((test_data$charges - mean(test_data$charges))^2)
r_squared <- 1 - (ss_res / ss_tot)

cat("Test RMSE:", rmse_val, "\n")
cat("Test R-squared:", r_squared, "\n")

rpart.plot(model, type = 2, extra = 101, fallen.leaves = TRUE)

plot(test_data$charges, preds, main = "Actual vs Predicted",
     xlab = "Actual Charges", ylab = "Predicted Charges")
abline(0, 1, col = "red")
```

Random Forest model

```{r}
rf_model <- randomForest(charges ~ ., data = train_data, ntree = 500)
preds <- predict(rf_model, newdata = test_data)

rmse_val <- rmse(test_data$charges, preds)
ss_res <- sum((test_data$charges - preds)^2)
ss_tot <- sum((test_data$charges - mean(test_data$charges))^2)
r_squared <- 1 - (ss_res / ss_tot)

cat("Test RMSE:", rmse_val, "\n")
cat("Test R-squared:", r_squared, "\n")
 
plot(test_data$charges, preds, main = "Actual vs Predicted",
     xlab = "Actual Charges", ylab = "Predicted Charges")
abline(0, 1, col = "red")
```
```{r}
importance(rf_model)
```
```{r}
ImpData <- as.data.frame(importance(rf_model))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`IncNodePurity`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`IncNodePurity`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```

```{r}
rf_pred <- predict(rf_model, , newdata = df)
rf_resid <- df$charges - rf_pred

plot(rf_pred, rf_resid,
     xlab = "Predicted", ylab = "Residuals", main = "Residuals vs Predicted (RF)")
abline(h = 0, col = "red")
```



